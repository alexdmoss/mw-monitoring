---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: mw
    role: k8s-alert-rules
  name: k8s-alert-rules
  namespace: prometheus
spec:
  groups:
    - name: kubernetes-absent
      rules:
        - alert: AlertmanagerDown
          annotations:
            message: Alertmanager has disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="alertmanager-mw",namespace="prometheus"} == 1)
          for: 15m
          labels:
            severity: critical
        - alert: KubeStateMetricsDown
          annotations:
            message: KubeStateMetrics has disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="kube-state-metrics"} == 1)
          for: 15m
          labels:
            severity: critical
        - alert: NodeExporterDown
          annotations:
            message: NodeExporter has disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="node-exporter"} == 1)
          for: 15m
          labels:
            severity: critical
        - alert: PrometheusDown
          annotations:
            message: Prometheus has disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="prometheus-mw",namespace="prometheus"} == 1)
          for: 15m
          labels:
            severity: critical
        - alert: PrometheusOperatorDown
          annotations:
            message: PrometheusOperator has disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="prometheus-operator",namespace="prometheus-operator"} == 1)
          for: 15m
          labels:
            severity: critical
    - name: kubernetes-apps
      rules:
        - alert: KubePodCrashLooping
          annotations:
            message:
              Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
              }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
          expr: |
            rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) * 60 * 5 > 0
          for: 1h
          labels:
            severity: critical
        - alert: KubePodNotReady
          annotations:
            message:
              Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
              state for longer than an hour.
          expr: |
            sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown"}) > 0
          for: 1h
          labels:
            severity: critical
        - alert: KubeDeploymentGenerationMismatch
          annotations:
            message:
              Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
              }} does not match, this indicates that the Deployment has failed but has
              not been rolled back.
          expr: |
            kube_deployment_status_observed_generation{job="kube-state-metrics"}
              !=
            kube_deployment_metadata_generation{job="kube-state-metrics"}
          for: 15m
          labels:
            severity: critical
        - alert: KubeDeploymentReplicasMismatch
          annotations:
            message:
              Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
              matched the expected number of replicas for longer than an hour.
          expr: |
            kube_deployment_spec_replicas{job="kube-state-metrics"}
              !=
            kube_deployment_status_replicas_available{job="kube-state-metrics"}
          for: 1h
          labels:
            severity: critical
        - alert: KubeStatefulSetReplicasMismatch
          annotations:
            message:
              StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
              not matched the expected number of replicas for longer than 15 minutes.
          expr: |
            kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas{job="kube-state-metrics"}
          for: 15m
          labels:
            severity: critical
        - alert: KubeStatefulSetGenerationMismatch
          annotations:
            message:
              StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
              }} does not match, this indicates that the StatefulSet has failed but has
              not been rolled back.
          expr: |
            kube_statefulset_status_observed_generation{job="kube-state-metrics"}
              !=
            kube_statefulset_metadata_generation{job="kube-state-metrics"}
          for: 15m
          labels:
            severity: critical
        - alert: KubeStatefulSetUpdateNotRolledOut
          annotations:
            message:
              StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
              has not been rolled out.
          expr: |
            max without (revision) (
              kube_statefulset_status_current_revision{job="kube-state-metrics"}
                unless
              kube_statefulset_status_update_revision{job="kube-state-metrics"}
            )
              *
            (
              kube_statefulset_replicas{job="kube-state-metrics"}
                !=
              kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
            )
          for: 15m
          labels:
            severity: critical
        - alert: KubeDaemonSetRolloutStuck
          annotations:
            message:
              Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
              }}/{{ $labels.daemonset }} are scheduled and ready.
          expr: |
            kube_daemonset_status_number_ready{job="kube-state-metrics"}
              /
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} * 100 < 100
          for: 15m
          labels:
            severity: critical
        - alert: KubeDaemonSetNotScheduled
          annotations:
            message:
              "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
              }} are not scheduled."
          expr: |
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              -
            kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
          for: 10m
          labels:
            severity: warning
        - alert: KubeDaemonSetMisScheduled
          annotations:
            message:
              "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
              }} are running where they are not supposed to run."
          expr: |
            kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
          for: 10m
          labels:
            severity: warning
        - alert: KubeCronJobRunning
          annotations:
            message:
              CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more
              than 1h to complete.
          expr: |
            time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
          for: 1h
          labels:
            severity: warning
        - alert: KubeJobCompletion
          annotations:
            message:
              Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
              than one hour to complete.
          expr: |
            kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
          for: 1h
          labels:
            severity: warning
        - alert: KubeJobFailed
          annotations:
            message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
          expr: |
            kube_job_status_failed{job="kube-state-metrics"}  > 0
          for: 1h
          labels:
            severity: warning
    - name: kubernetes-resources
      rules:
        - alert: KubeCPUOvercommit
          annotations:
            message:
              Cluster has overcommitted CPU resource requests for Pods and cannot
              tolerate node failure.
          expr: |
            sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
              /
            sum(kube_node_status_allocatable_cpu_cores)
              >
            (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
          for: 5m
          labels:
            severity: warning
        - alert: KubeMemOvercommit
          annotations:
            message:
              Cluster has overcommitted memory resource requests for Pods and cannot
              tolerate node failure.
          expr: |
            sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
              /
            sum(kube_node_status_allocatable_memory_bytes)
              >
            (count(kube_node_status_allocatable_memory_bytes)-1)
              /
            count(kube_node_status_allocatable_memory_bytes)
          for: 5m
          labels:
            severity: warning
        - alert: KubeCPUOvercommit
          annotations:
            message: Cluster has overcommitted CPU resource requests for Namespaces.
          expr: |
            sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
              /
            sum(kube_node_status_allocatable_cpu_cores)
              > 1.5
          for: 5m
          labels:
            severity: warning
        - alert: KubeMemOvercommit
          annotations:
            message: Cluster has overcommitted memory resource requests for Namespaces.
          expr: |
            sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
              /
            sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
              > 1.5
          for: 5m
          labels:
            severity: warning
        - alert: KubeQuotaExceeded
          annotations:
            message:
              Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value
              }}% of its {{ $labels.resource }} quota.
          expr: |
            100 * kube_resourcequota{job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
              > 90
          for: 15m
          labels:
            severity: warning
        - alert: CPUThrottlingHigh
          annotations:
            message:
              '{{ printf "%0.0f" $value }}% throttling of CPU in namespace {{ $labels.namespace
              }} for container {{ $labels.container }} in pod {{ $labels.pod }}.'
          expr:
            "100 * sum(increase(container_cpu_cfs_throttled_periods_total{container!=\"\",
            }[5m])) by (container, pod, namespace)\n  /\nsum(increase(container_cpu_cfs_periods_total{}[5m]))
            by (container, pod, namespace)\n  > 25 \n"
          for: 15m
          labels:
            severity: warning
    - name: kubernetes-storage
      rules:
        - alert: KubePersistentVolumeUsageCritical
          annotations:
            message:
              The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.2f" $value
              }}% free.
          expr: |
            100 * kubelet_volume_stats_available_bytes{job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubelet"}
              < 3
          for: 1m
          labels:
            severity: critical
        - alert: KubePersistentVolumeFullInFourDays
          annotations:
            message:
              Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is expected to fill up within four
              days. Currently {{ printf "%0.2f" $value }}% is available.
          expr: |
            100 * (
              kubelet_volume_stats_available_bytes{job="kubelet"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubelet"}
            ) < 15
            and
            predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
          for: 5m
          labels:
            severity: critical
        - alert: KubePersistentVolumeErrors
          annotations:
            message:
              The persistent volume {{ $labels.persistentvolume }} has status {{
              $labels.phase }}.
          expr: |
            kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
          for: 5m
          labels:
            severity: critical
    - name: kubernetes-system
      rules:
        - alert: KubeNodeNotReady
          annotations:
            message: "{{ $labels.node }} has been unready for more than an hour."
          expr: |
            kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
          for: 1h
          labels:
            severity: warning
        - alert: KubeClientErrors
          annotations:
            message:
              Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
              }}' is experiencing {{ printf "%0.0f" $value }}% errors.'
          expr: |
            (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
              /
            sum(rate(rest_client_requests_total[5m])) by (instance, job))
            * 100 > 1
          for: 15m
          labels:
            severity: warning
        - alert: KubeClientErrors
          annotations:
            message:
              Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
              }}' is experiencing {{ printf "%0.0f" $value }} errors / second.'
          expr: |
            sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
          for: 15m
          labels:
            severity: warning
    - name: alertmanager.rules
      rules:
        - alert: AlertmanagerConfigInconsistent
          annotations:
            message:
              The configuration of the instances of the Alertmanager cluster `{{$labels.service}}`
              are out of sync.
          expr: |
            count_values("config_hash", alertmanager_config_hash{job="alertmanager-mw",namespace="prometheus"}) BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_spec_replicas{job="prometheus-operator",namespace="prometheus-operator",controller="alertmanager"}, "service", "alertmanager-$1", "name", "(.*)") != 1
          for: 5m
          labels:
            severity: critical
        - alert: AlertmanagerFailedReload
          annotations:
            message:
              Reloading Alertmanager's configuration has failed for {{ $labels.namespace
              }}/{{ $labels.pod}}.
          expr: |
            alertmanager_config_last_reload_successful{job="alertmanager-mw",namespace="prometheus"} == 0
          for: 10m
          labels:
            severity: warning
        - alert: AlertmanagerMembersInconsistent
          annotations:
            message: Alertmanager has not found all other members of the cluster.
          expr: |
            alertmanager_cluster_members{job="alertmanager-mw",namespace="prometheus"}
              != on (service) GROUP_LEFT()
            count by (service) (alertmanager_cluster_members{job="alertmanager-mw",namespace="prometheus"})
          for: 5m
          labels:
            severity: critical
    - name: kube-prometheus-node-alerting.rules
      rules:
        - alert: NodeDiskRunningFull
          annotations:
            message:
              Device {{ $labels.device }} of node-exporter {{ $labels.namespace
              }}/{{ $labels.pod }} will be full within the next 24 hours.
          expr: |
            (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[6h], 3600 * 24) < 0)
          for: 30m
          labels:
            severity: warning
        - alert: NodeDiskRunningFull
          annotations:
            message:
              Device {{ $labels.device }} of node-exporter {{ $labels.namespace
              }}/{{ $labels.pod }} will be full within the next 2 hours.
          expr: |
            (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[30m], 3600 * 2) < 0)
          for: 10m
          labels:
            severity: critical
    - name: node-time
      rules:
        - alert: ClockSkewDetected
          annotations:
            message:
              Clock skew detected on node-exporter {{ $labels.namespace }}/{{ $labels.pod
              }}. Ensure NTP is configured correctly on this host.
          expr: |
            abs(node_timex_offset_seconds{job="node-exporter"}) > 0.03
          for: 2m
          labels:
            severity: warning
    - name: node-network
      rules:
        - alert: NetworkReceiveErrors
          annotations:
            message:
              Network interface "{{ $labels.device }}" showing receive errors on
              node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
          expr: |
            rate(node_network_receive_errs_total{job="node-exporter",device!~"veth.+"}[2m]) > 0
          for: 2m
          labels:
            severity: warning
        - alert: NetworkTransmitErrors
          annotations:
            message:
              Network interface "{{ $labels.device }}" showing transmit errors
              on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
          expr: |
            rate(node_network_transmit_errs_total{job="node-exporter",device!~"veth.+"}[2m]) > 0
          for: 2m
          labels:
            severity: warning
        - alert: NodeNetworkInterfaceFlapping
          annotations:
            message:
              Network interface "{{ $labels.device }}" changing it's up status
              often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
          expr: |
            changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
          for: 2m
          labels:
            severity: warning
    - name: prometheus.rules
      rules:
        - alert: PrometheusConfigReloadFailed
          annotations:
            description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
            summary: Reloading Prometheus' configuration failed
          expr: |
            prometheus_config_last_reload_successful{job="prometheus-mw",namespace="prometheus"} == 0
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusNotificationQueueRunningFull
          annotations:
            description:
              Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{
              $labels.pod}}
            summary: Prometheus' alert notification queue is running full
          expr: |
            predict_linear(prometheus_notifications_queue_length{job="prometheus-mw",namespace="prometheus"}[5m], 60 * 30) > prometheus_notifications_queue_capacity{job="prometheus-mw",namespace="prometheus"}
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusErrorSendingAlerts
          annotations:
            description:
              Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
              $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
            summary: Errors while sending alert from Prometheus
          expr: |
            rate(prometheus_notifications_errors_total{job="prometheus-mw",namespace="prometheus"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus-mw",namespace="prometheus"}[5m]) > 0.01
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusErrorSendingAlerts
          annotations:
            description:
              Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
              $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
            summary: Errors while sending alerts from Prometheus
          expr: |
            rate(prometheus_notifications_errors_total{job="prometheus-mw",namespace="prometheus"}[5m]) / rate(prometheus_notifications_sent_total{job="prometheus-mw",namespace="prometheus"}[5m]) > 0.03
          for: 10m
          labels:
            severity: critical
        - alert: PrometheusNotConnectedToAlertmanagers
          annotations:
            description:
              Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected
              to any Alertmanagers
            summary: Prometheus is not connected to any Alertmanagers
          expr: |
            prometheus_notifications_alertmanagers_discovered{job="prometheus-mw",namespace="prometheus"} < 1
          for: 10m
          labels:
            severity: warning
        # [erroring loading config - unclear why]
        - alert: PrometheusTSDBReloadsFailing
          annotations:
            description:
              "{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
              reload failures over the last four hours."
            summary: Prometheus has issues reloading data blocks from disk
          expr: |
            increase(prometheus_tsdb_reloads_failures_total{job="prometheus-mw",namespace="prometheus"}[2h]) > 0
          for: 12h
          labels:
            severity: warning
        - alert: PrometheusTSDBCompactionsFailing
          annotations:
            description:
              "{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
              compaction failures over the last four hours."
            summary: Prometheus has issues compacting sample blocks
          expr: |
            increase(prometheus_tsdb_compactions_failed_total{job="prometheus-mw",namespace="prometheus"}[2h]) > 0
          for: 12h
          labels:
            severity: warning
        - alert: PrometheusTSDBWALCorruptions
          annotations:
            description:
              "{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead
              log (WAL)."
            summary: Prometheus write-ahead log is corrupted
          expr: |
            prometheus_tsdb_wal_corruptions_total{job="prometheus-mw",namespace="prometheus"} > 0
          for: 4h
          labels:
            severity: warning
        - alert: PrometheusNotIngestingSamples
          annotations:
            description:
              Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting
              samples.
            summary: Prometheus isn't ingesting samples
          expr: |
            rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-mw",namespace="prometheus"}[5m]) <= 0
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusTargetScrapesDuplicate
          annotations:
            description:
              "{{$labels.namespace}}/{{$labels.pod}} has many samples rejected
              due to duplicate timestamps but different values"
            summary: Prometheus has many samples rejected
          expr: |
            increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-mw",namespace="prometheus"}[5m]) > 0
          for: 10m
          labels:
            severity: warning
    - name: prometheus-operator
      rules:
        - alert: PrometheusOperatorReconcileErrors
          annotations:
            message:
              Errors while reconciling {{ $labels.controller }} in {{ $labels.namespace
              }} Namespace.
          expr: |
            rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator",namespace="prometheus-operator"}[5m]) > 0.1
          for: 10m
          labels:
            severity: warning
        - alert: PrometheusOperatorNodeLookupErrors
          annotations:
            message: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
          expr: |
            rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator",namespace="prometheus-operator"}[5m]) > 0.1
          for: 10m
          labels:
            severity: warning
